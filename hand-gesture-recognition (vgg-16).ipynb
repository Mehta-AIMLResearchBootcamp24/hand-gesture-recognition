{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61155,"sourceType":"datasetVersion","datasetId":39466}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''images=[]\nlabels=[]\n\n# loop over top 10 folders\nfor directory in os.listdir('/kaggle/input/leapgestrecog/leapGestRecog/'):\n    for subdir in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/',directory)):\n#         if directory=='01':\n#             labels.append(subdir)\n        for image in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/',directory, subdir)):\n            img_path = os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/', directory, subdir, image)\n            images.append(img_path)\n            labels.append(subdir)'''\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:16.985291Z","iopub.execute_input":"2024-07-25T04:23:16.985982Z","iopub.status.idle":"2024-07-25T04:23:16.997396Z","shell.execute_reply.started":"2024-07-25T04:23:16.985930Z","shell.execute_reply":"2024-07-25T04:23:16.995319Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"\"images=[]\\nlabels=[]\\n\\n# loop over top 10 folders\\nfor directory in os.listdir('/kaggle/input/leapgestrecog/leapGestRecog/'):\\n    for subdir in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/',directory)):\\n#         if directory=='01':\\n#             labels.append(subdir)\\n        for image in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/',directory, subdir)):\\n            img_path = os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/', directory, subdir, image)\\n            images.append(img_path)\\n            labels.append(subdir)\""},"metadata":{}}]},{"cell_type":"code","source":"import os \n\ntraining_images = []\ntraining_labels = []\ntesting_images = []\ntesting_labels = []\n\n# loop over top 10 folders\nfor directory in os.listdir('/kaggle/input/leapgestrecog/leapGestRecog/'):\n    for subdir in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/', directory)):\n        for image in os.listdir(os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/', directory, subdir)):\n            img_path = os.path.join('/kaggle/input/leapgestrecog/leapGestRecog/', directory, subdir, image)\n            \n            # Split into training and testing \n            if int(directory) <= 7:\n                training_images.append(img_path)\n                training_labels.append(subdir)\n            else:\n                testing_images.append(img_path)\n                testing_labels.append(subdir)\n\nprint(\"Number of training images:\", len(training_images))\nprint(\"Number of training labels:\", len(training_labels))\nprint(\"Number of testing images:\", len(testing_images))\nprint(\"Number of testing labels:\", len(testing_labels))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:16.999814Z","iopub.execute_input":"2024-07-25T04:23:17.000412Z","iopub.status.idle":"2024-07-25T04:23:17.168835Z","shell.execute_reply.started":"2024-07-25T04:23:17.000375Z","shell.execute_reply":"2024-07-25T04:23:17.167988Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Number of training images: 16000\nNumber of training labels: 16000\nNumber of testing images: 4000\nNumber of testing labels: 4000\n","output_type":"stream"}]},{"cell_type":"code","source":"label_to_integer = dict(zip(sorted(list(set(training_labels))), range(len(training_labels))))\nlabel_to_integer","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.170005Z","iopub.execute_input":"2024-07-25T04:23:17.170432Z","iopub.status.idle":"2024-07-25T04:23:17.177684Z","shell.execute_reply.started":"2024-07-25T04:23:17.170399Z","shell.execute_reply":"2024-07-25T04:23:17.176748Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'01_palm': 0,\n '02_l': 1,\n '03_fist': 2,\n '04_fist_moved': 3,\n '05_thumb': 4,\n '06_index': 5,\n '07_ok': 6,\n '08_palm_moved': 7,\n '09_c': 8,\n '10_down': 9}"},"metadata":{}}]},{"cell_type":"code","source":"training_int_labels = list(map(lambda x: label_to_integer[x], training_labels))\ntesting_int_labels = list(map(lambda x: label_to_integer[x], testing_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.180171Z","iopub.execute_input":"2024-07-25T04:23:17.180446Z","iopub.status.idle":"2024-07-25T04:23:17.187968Z","shell.execute_reply.started":"2024-07-25T04:23:17.180423Z","shell.execute_reply":"2024-07-25T04:23:17.187224Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport glob\nimport os\n\n# Prepare end indices for samplers\ntrain_end_idx = []\ntest_end_idx = []\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.188946Z","iopub.execute_input":"2024-07-25T04:23:17.189273Z","iopub.status.idle":"2024-07-25T04:23:17.197982Z","shell.execute_reply.started":"2024-07-25T04:23:17.189250Z","shell.execute_reply":"2024-07-25T04:23:17.197030Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for different plot\nfrom sklearn.preprocessing import OneHotEncoder #for data preprocessing\nfrom sklearn.preprocessing import StandardScaler #for scaling data\nfrom sklearn.model_selection import train_test_split\n#for data preprocessing using keras\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# keras classes required for building deep CNN model\nfrom keras.models import Sequential, save_model, load_model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout \nfrom keras.utils import plot_model\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.199129Z","iopub.execute_input":"2024-07-25T04:23:17.199680Z","iopub.status.idle":"2024-07-25T04:23:17.208738Z","shell.execute_reply.started":"2024-07-25T04:23:17.199648Z","shell.execute_reply":"2024-07-25T04:23:17.207875Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Group images by subdirectory (gesture) and count\nfor directory in range(10):\n    dir_path = f'/kaggle/input/leapgestrecog/leapGestRecog/{directory:02d}'\n    for subdir in os.listdir(dir_path):\n        subdir_path = os.path.join(dir_path, subdir)\n        if os.path.isdir(subdir_path):\n            paths = glob.glob(os.path.join(subdir_path, '*.png'))\n            if directory <= 7:\n                train_end_idx.append(len(paths))\n            else:\n                test_end_idx.append(len(paths))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.209908Z","iopub.execute_input":"2024-07-25T04:23:17.210462Z","iopub.status.idle":"2024-07-25T04:23:17.338081Z","shell.execute_reply.started":"2024-07-25T04:23:17.210431Z","shell.execute_reply":"2024-07-25T04:23:17.337325Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# this code block divides into starting and ending of a specific gesture \n#converted into tensors \n\ntrain_end_idx = [0] + train_end_idx\ntrain_end_idx = torch.cumsum(torch.tensor(train_end_idx), 0)\n\ntest_end_idx = [0] + test_end_idx\ntest_end_idx = torch.cumsum(torch.tensor(test_end_idx), 0)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.339172Z","iopub.execute_input":"2024-07-25T04:23:17.339508Z","iopub.status.idle":"2024-07-25T04:23:17.345386Z","shell.execute_reply.started":"2024-07-25T04:23:17.339478Z","shell.execute_reply":"2024-07-25T04:23:17.344343Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#sequence of images \nseq_length = 20\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.349102Z","iopub.execute_input":"2024-07-25T04:23:17.349826Z","iopub.status.idle":"2024-07-25T04:23:17.355550Z","shell.execute_reply.started":"2024-07-25T04:23:17.349793Z","shell.execute_reply":"2024-07-25T04:23:17.354695Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"test_end_idx = [0, *test_end_idx]\ntest_end_idx = torch.cumsum(torch.tensor(test_end_idx), 0)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.356545Z","iopub.execute_input":"2024-07-25T04:23:17.356789Z","iopub.status.idle":"2024-07-25T04:23:17.364590Z","shell.execute_reply.started":"2024-07-25T04:23:17.356758Z","shell.execute_reply":"2024-07-25T04:23:17.363834Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass MySampler(torch.utils.data.Sampler):\n    def __init__(self, end_idx, seq_length):\n        indices = []\n        for i in range(len(end_idx)-1):\n            start = end_idx[i]\n            # end = end_idx[i+1] - seq_length\n            end = end_idx[i+1] - seq_length\n            if (end != -20 and end <= 15999):\n                indices.append(torch.arange(start, end))\n        indices = torch.cat(indices)\n\n        self.indices = indices\n\n    def __iter__(self):\n        indices = self.indices[torch.randperm(len(self.indices))]\n        return iter(indices.tolist())\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass MyDataset(Dataset):\n    def __init__(self, image_paths, labels, seq_length, transform, length, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.seq_length = seq_length\n        self.transform = transform\n        self.length = length\n        self.device = device\n        self.n_frames = 100\n\n\n    def __getitem__(self, index):\n        start = index\n        # end = index + self.seq_length\n        end = index + self.n_frames\n        # print('Getting images from {} to {}'.format(start, end))\n        indices = list(range(start, end))\n        images = []\n        #print(start, end)\n        #print(len(self.image_paths))\n        for i in indices:\n            image_path = self.image_paths[i]\n            # print(image_path)\n            # print('image_path :', image_path)\n            image = Image.open(image_path)\n            image = image.convert(\"RGB\")\n            if self.transform:\n                image = self.transform(image)\n            images.append(image)\n        # print('=============================')\n\n        #getting the video ID\n        video_id = self.image_paths[i][0]\n        vid = video_id.split('/')[-2]\n\n        x = torch.stack(images).to(self.device)\n\n        y = self.labels[index]\n        return x, y\n\n    def __len__(self):\n        return self.length","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.366111Z","iopub.execute_input":"2024-07-25T04:23:17.366492Z","iopub.status.idle":"2024-07-25T04:23:17.381020Z","shell.execute_reply.started":"2024-07-25T04:23:17.366454Z","shell.execute_reply":"2024-07-25T04:23:17.379659Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# samplers creation here\nimport torchvision.transforms as transforms\n\ntrain_sampler = MySampler(train_end_idx, 20)\ntest_sampler = MySampler(test_end_idx, seq_length)\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.225])\n])\n\n\n# defining the datset again in here\ntrain_data = MyDataset(image_paths=training_images,\n                       labels=training_int_labels,\n                       seq_length=seq_length,\n                       transform = transform,\n                       length=len(train_sampler))\n\ntest_data = MyDataset(image_paths=testing_images,\n                      labels=testing_int_labels,\n                      seq_length=seq_length,\n                      transform = transform,\n                      length=len(test_sampler))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.382264Z","iopub.execute_input":"2024-07-25T04:23:17.382562Z","iopub.status.idle":"2024-07-25T04:23:17.398802Z","shell.execute_reply.started":"2024-07-25T04:23:17.382540Z","shell.execute_reply":"2024-07-25T04:23:17.398009Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"batch_size = 10\ntrain_loader = DataLoader(dataset= train_data, batch_size=batch_size,sampler=train_sampler)\ntest_loader = DataLoader(dataset= test_data, batch_size=batch_size, sampler=test_sampler)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.399863Z","iopub.execute_input":"2024-07-25T04:23:17.400158Z","iopub.status.idle":"2024-07-25T04:23:17.405265Z","shell.execute_reply.started":"2024-07-25T04:23:17.400136Z","shell.execute_reply":"2024-07-25T04:23:17.404197Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"#include both image path and labels into dataframe\n#df = pd.DataFrame({'Image':images,'Label':labels})\n#df.tail()\n# df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.406375Z","iopub.execute_input":"2024-07-25T04:23:17.406653Z","iopub.status.idle":"2024-07-25T04:23:17.415006Z","shell.execute_reply.started":"2024-07-25T04:23:17.406631Z","shell.execute_reply":"2024-07-25T04:23:17.414103Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"'''unique_labels = np.unique(df['Label'])\nlabel_to_integer = dict(zip(unique_labels, range(len(unique_labels))))\n\ndf['LabelInt'] = df['Label'].map(label_to_integer)'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.416108Z","iopub.execute_input":"2024-07-25T04:23:17.416437Z","iopub.status.idle":"2024-07-25T04:23:17.425820Z","shell.execute_reply.started":"2024-07-25T04:23:17.416407Z","shell.execute_reply":"2024-07-25T04:23:17.424813Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"\"unique_labels = np.unique(df['Label'])\\nlabel_to_integer = dict(zip(unique_labels, range(len(unique_labels))))\\n\\ndf['LabelInt'] = df['Label'].map(label_to_integer)\""},"metadata":{}}]},{"cell_type":"code","source":"'''df['Label'] = df['LabelInt']'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.426849Z","iopub.execute_input":"2024-07-25T04:23:17.428275Z","iopub.status.idle":"2024-07-25T04:23:17.435967Z","shell.execute_reply.started":"2024-07-25T04:23:17.428241Z","shell.execute_reply":"2024-07-25T04:23:17.434941Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"\"df['Label'] = df['LabelInt']\""},"metadata":{}}]},{"cell_type":"code","source":"'''df = df.drop(columns=[\"LabelInt\"])\ndf'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.437027Z","iopub.execute_input":"2024-07-25T04:23:17.437283Z","iopub.status.idle":"2024-07-25T04:23:17.447304Z","shell.execute_reply.started":"2024-07-25T04:23:17.437261Z","shell.execute_reply":"2024-07-25T04:23:17.446529Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'df = df.drop(columns=[\"LabelInt\"])\\ndf'"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# from torch.utils.data import Dataset, DataLoader, ConcatDataset\n# from PIL import Image\n# from torchvision import datasets, models, transforms\n# import torchvision\n# import numpy as np\n# class CNNDataset(Dataset):\n#     \"\"\"\n#     Custom dataset definition\n#     \"\"\"\n#     def __init__(self, df, mode='train', transform=None):\n#         \"\"\"\n#         \"\"\"\n#         self.df = df\n#         self.mode= mode\n#         self.transform = transform\n        \n            \n#     def __getitem__(self, index):\n#         \"\"\"\n#         \"\"\"\n#         img_name = self.df.iloc[index][\"Image\"]\n#         image = Image.open(img_name)\n                \n#         if self.mode == 'train':\n#             image = image.convert(\"RGB\")\n                \n#             image = np.asarray(image)\n#             if self.transform is not None:\n#                 image = self.transform(image)\n#             labels =  np.asarray(self.df.iloc[index][\"Label\"]) #might not need this\n#             return image, labels\n            \n#         elif self.mode == 'val':\n#             image = image.convert(\"RGB\")\n            \n#             image = np.asarray(image)\n#             if self.transform is not None:\n#                 image = self.transform(image)\n#             labels = np.asarray(self.df.iloc[index][\"Label\"])\n#             return image, labels\n        \n#     def __len__(self):\n#         return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.448398Z","iopub.execute_input":"2024-07-25T04:23:17.448740Z","iopub.status.idle":"2024-07-25T04:23:17.456628Z","shell.execute_reply.started":"2024-07-25T04:23:17.448709Z","shell.execute_reply":"2024-07-25T04:23:17.455866Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"''' def get_dataloaders(input_size, batch_size, augment=False, shuffle = True):\n    # How to transform the image when you are loading them.\n    # you'll likely want to mess with the transforms on the training set.\n    \n    # For now, we resize/crop the image to the correct input size for our network,\n    # then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n    # are derived from aggregating lots of data and happen to produce better results.\n    data_transforms = {\n        'train': transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(input_size),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.225])\n        ]),\n        'val': transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(input_size),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.225])\n        ])\n    }\n    \n    data_subsets = {x: CNNDataset(df, \n                                  mode = x,\n                                  transform=data_transforms[x]) for x in data_transforms.keys()}\n    # Create training and validation dataloaders\n    # Never shuffle the test set\n    dataloaders_dict = {x: DataLoader(data_subsets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\n    return dataloaders_dict\n '''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.457769Z","iopub.execute_input":"2024-07-25T04:23:17.458098Z","iopub.status.idle":"2024-07-25T04:23:17.472914Z","shell.execute_reply.started":"2024-07-25T04:23:17.458074Z","shell.execute_reply":"2024-07-25T04:23:17.472061Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"\" def get_dataloaders(input_size, batch_size, augment=False, shuffle = True):\\n    # How to transform the image when you are loading them.\\n    # you'll likely want to mess with the transforms on the training set.\\n    \\n    # For now, we resize/crop the image to the correct input size for our network,\\n    # then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\\n    # are derived from aggregating lots of data and happen to produce better results.\\n    data_transforms = {\\n        'train': transforms.Compose([\\n            transforms.ToPILImage(),\\n            transforms.Resize(input_size),\\n            transforms.CenterCrop(input_size),\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5], [0.225])\\n        ]),\\n        'val': transforms.Compose([\\n            transforms.ToPILImage(),\\n            transforms.Resize(input_size),\\n            transforms.CenterCrop(input_size),\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5], [0.225])\\n        ])\\n    }\\n    \\n    data_subsets = {x: CNNDataset(df, \\n                                  mode = x,\\n                                  transform=data_transforms[x]) for x in data_transforms.keys()}\\n    # Create training and validation dataloaders\\n    # Never shuffle the test set\\n    dataloaders_dict = {x: DataLoader(data_subsets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\\n    return dataloaders_dict\\n \""},"metadata":{}}]},{"cell_type":"code","source":"'''import torch\nimport torchvision.models as models\n\n# Load the pre-trained ResNet-18 model\nmodel = models.vgg16(pretrained=True)\n\n# Freeze all the pre-trained layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n\n# Modify the last layer of the model\nnum_classes = 10 # replace with the number of classes in your dataset\n\n#lastlayer = model.classifier[6]\n#num_ftrs = model.fc.in_features\nnum_ftrs = model.classifier[6].in_features\n#model.fc = torch.nn.Linear(num_ftrs, num_classes)\nmodel.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.474107Z","iopub.execute_input":"2024-07-25T04:23:17.474376Z","iopub.status.idle":"2024-07-25T04:23:17.486053Z","shell.execute_reply.started":"2024-07-25T04:23:17.474354Z","shell.execute_reply":"2024-07-25T04:23:17.484997Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"'import torch\\nimport torchvision.models as models\\n\\n# Load the pre-trained ResNet-18 model\\nmodel = models.vgg16(pretrained=True)\\n\\n# Freeze all the pre-trained layers\\nfor param in model.parameters():\\n    param.requires_grad = False\\n\\n\\n# Modify the last layer of the model\\nnum_classes = 10 # replace with the number of classes in your dataset\\n\\n#lastlayer = model.classifier[6]\\n#num_ftrs = model.fc.in_features\\nnum_ftrs = model.classifier[6].in_features\\n#model.fc = torch.nn.Linear(num_ftrs, num_classes)\\nmodel.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)'"},"metadata":{}}]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\n\n'''# Define the loss function and optimizer\n#optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\noptimizer = torch.optim.SGD(model.classifier[6].parameters(), lr=0.001, momentum=0.9)'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.487140Z","iopub.execute_input":"2024-07-25T04:23:17.487413Z","iopub.status.idle":"2024-07-25T04:23:17.499505Z","shell.execute_reply.started":"2024-07-25T04:23:17.487391Z","shell.execute_reply":"2024-07-25T04:23:17.498709Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'# Define the loss function and optimizer\\n#optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\\noptimizer = torch.optim.SGD(model.classifier[6].parameters(), lr=0.001, momentum=0.9)'"},"metadata":{}}]},{"cell_type":"code","source":"'''dataloaders = get_dataloaders(input_size=256, batch_size=64, shuffle=True)'''\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.500780Z","iopub.execute_input":"2024-07-25T04:23:17.501357Z","iopub.status.idle":"2024-07-25T04:23:17.510378Z","shell.execute_reply.started":"2024-07-25T04:23:17.501327Z","shell.execute_reply":"2024-07-25T04:23:17.509588Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"'dataloaders = get_dataloaders(input_size=256, batch_size=64, shuffle=True)'"},"metadata":{}}]},{"cell_type":"code","source":"'''train_loader = dataloaders['train']\nval_loader = dataloaders['val']'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.511309Z","iopub.execute_input":"2024-07-25T04:23:17.511573Z","iopub.status.idle":"2024-07-25T04:23:17.521002Z","shell.execute_reply.started":"2024-07-25T04:23:17.511550Z","shell.execute_reply":"2024-07-25T04:23:17.520166Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"\"train_loader = dataloaders['train']\\nval_loader = dataloaders['val']\""},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNNLSTM(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNNLSTM, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 300))\n        self.lstm = nn.LSTM(input_size=300, hidden_size=256, num_layers=3)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n    def forward(self, x_3d):\n        hidden = None\n        # Iterate over each frame of a video in a video of batch * frames * channels * height * width\n        for t in range(x_3d.size(1)):\n            with torch.no_grad():\n                x = self.resnet(x_3d[:, t])\n            # Pass latent representation of frame through lstm and update hidden state\n            out, hidden = self.lstm(x.unsqueeze(0), hidden)\n        # Get the last hidden state (hidden is a tuple with both hidden and cell state in it)\n        x = self.fc1(hidden[0][-1])\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.525780Z","iopub.execute_input":"2024-07-25T04:23:17.526105Z","iopub.status.idle":"2024-07-25T04:23:17.535844Z","shell.execute_reply.started":"2024-07-25T04:23:17.526074Z","shell.execute_reply":"2024-07-25T04:23:17.534811Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#shifting model to gpu\n#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.537106Z","iopub.execute_input":"2024-07-25T04:23:17.537462Z","iopub.status.idle":"2024-07-25T04:23:17.548191Z","shell.execute_reply.started":"2024-07-25T04:23:17.537431Z","shell.execute_reply":"2024-07-25T04:23:17.547400Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"num_classes = 10\n\n#Different model parameters to play around with\nnum_epochs = 4\nlearning_rate = 0.01","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.549153Z","iopub.execute_input":"2024-07-25T04:23:17.549411Z","iopub.status.idle":"2024-07-25T04:23:17.557779Z","shell.execute_reply.started":"2024-07-25T04:23:17.549389Z","shell.execute_reply":"2024-07-25T04:23:17.556923Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def train():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model=CNNLSTM().to(device) #for the CNN LSTM\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    total_step = len(train_loader)\n    \n    #take out the following code if just starting to train\n    #checkpoint = torch.load('PREVIOUS MODEL PATH')  for ex: torch.load('../input/trained-model/model_21.pth')\n    #model.load_state_dict(checkpoint['state_dict'])\n    #optimizer.load_state_dict(checkpoint['optimizer'])\n    #epoch_before = checkpoint['epoch']\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(tqdm(train_loader), 1):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            outputs = torch.squeeze(outputs)\n            \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        #'epoch' = epoch if just starting to train\n        #'epoch' = epoch+epoch_before+1 afterwards\n        model_file = { 'epoch': epoch,\n                      'state_dict': model.state_dict(),\n                      'optimizer' : optimizer.state_dict()}\n\n        torch.save(model_file, \"model\" + str(epoch) + '.pth')  \n        #str(epoch) if just starting to train \n        #str(epoch+epoch_before+1) afterwards\n\n        model.eval()\n\n        train_correct = 0\n        train_total = 0\n        with torch.no_grad():\n            for data in tqdm(train_loader):\n                images, labels = data\n                \n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n\n                train_total += labels.size(0)\n\n                train_correct += (predicted == labels).sum().item()\n        print(train_correct, train_total)\n                \n        #str(epoch) if just starting to train\n        #str(epoch+epoch_before+1) afterwards\n        print(\"epoch: \" + str(epoch))\n        print('Top One Error of the network on train images: %d %%' % (\n                100 * (1 - float(train_correct) / train_total)))\n\n\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for data in tqdm(val_loader):\n                images, labels = data\n\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n\n                val_total += labels.size(0)\n\n                val_correct += (predicted == labels).sum().item()\n        print(val_correct, val_total)\n\n        #str(epoch) if just starting to train\n        #str(epoch+epoch_before+1) afterwards\n        print(\"epoch: \" + str(epoch))\n        print('Top One Error of the network on validation images: %d %%' % (\n                100 * (1 - float(val_correct) / val_total)))\n        \n\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.559165Z","iopub.execute_input":"2024-07-25T04:23:17.559446Z","iopub.status.idle":"2024-07-25T04:23:17.575318Z","shell.execute_reply.started":"2024-07-25T04:23:17.559416Z","shell.execute_reply":"2024-07-25T04:23:17.574448Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"'''def train():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model=CNNLSTM().to(device) #for the CNN LSTM\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    total_step = len(train_loader)\n    \n    #take out the following code if just starting to train\n    #checkpoint = torch.load('PREVIOUS MODEL PATH')  for ex: torch.load('../input/trained-model/model_21.pth')\n    #model.load_state_dict(checkpoint['state_dict'])\n    #optimizer.load_state_dict(checkpoint['optimizer'])\n    #epoch_before = checkpoint['epoch']\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(tqdm(train_loader), 1):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            outputs = torch.squeeze(outputs)\n            \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        #'epoch' = epoch if just starting to train\n        #'epoch' = epoch+epoch_before+1 afterwards\n        model_file = { 'epoch': epoch,\n                      'state_dict': model.state_dict(),\n                      'optimizer' : optimizer.state_dict()}\n\n        torch.save(model_file, \"model\" + str(epoch) + '.pth')  \n        #str(epoch) if just starting to train \n        #str(epoch+epoch_before+1) afterwards\n\n        model.eval()\n\n        train_correct = 0\n        train_total = 0\n        with torch.no_grad():\n            for data in tqdm(train_loader):\n                images, labels = data\n                \n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n\n                train_total += labels.size(0)\n\n                train_correct += (predicted == labels).sum().item()\n        print(train_correct, train_total)\n                \n        #str(epoch) if just starting to train\n        #str(epoch+epoch_before+1) afterwards\n        print(\"epoch: \" + str(epoch))\n        print('Top One Error of the network on train images: %d %%' % (\n                100 * (1 - float(train_correct) / train_total)))\n\n\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for data in tqdm(val_loader):\n                images, labels = data\n\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n\n                val_total += labels.size(0)\n\n                val_correct += (predicted == labels).sum().item()\n        print(val_correct, val_total)\n\n        #str(epoch) if just starting to train\n        #str(epoch+epoch_before+1) afterwards\n        print(\"epoch: \" + str(epoch))\n        print('Top One Error of the network on validation images: %d %%' % (\n                100 * (1 - float(val_correct) / val_total)))\n        \n\n        gc.collect()'''","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.576575Z","iopub.execute_input":"2024-07-25T04:23:17.576834Z","iopub.status.idle":"2024-07-25T04:23:17.592165Z","shell.execute_reply.started":"2024-07-25T04:23:17.576813Z","shell.execute_reply":"2024-07-25T04:23:17.591320Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"'def train():\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    model=CNNLSTM().to(device) #for the CNN LSTM\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\n\\n    total_step = len(train_loader)\\n    \\n    #take out the following code if just starting to train\\n    #checkpoint = torch.load(\\'PREVIOUS MODEL PATH\\')  for ex: torch.load(\\'../input/trained-model/model_21.pth\\')\\n    #model.load_state_dict(checkpoint[\\'state_dict\\'])\\n    #optimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\n    #epoch_before = checkpoint[\\'epoch\\']\\n    \\n    for epoch in range(num_epochs):\\n        running_loss = 0.0\\n        for i, (inputs, labels) in enumerate(tqdm(train_loader), 1):\\n            inputs = inputs.to(device)\\n            labels = labels.to(device)\\n            optimizer.zero_grad()\\n            \\n            outputs = model(inputs)\\n            outputs = torch.squeeze(outputs)\\n            \\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n            running_loss += loss.item()\\n        \\n        #\\'epoch\\' = epoch if just starting to train\\n        #\\'epoch\\' = epoch+epoch_before+1 afterwards\\n        model_file = { \\'epoch\\': epoch,\\n                      \\'state_dict\\': model.state_dict(),\\n                      \\'optimizer\\' : optimizer.state_dict()}\\n\\n        torch.save(model_file, \"model\" + str(epoch) + \\'.pth\\')  \\n        #str(epoch) if just starting to train \\n        #str(epoch+epoch_before+1) afterwards\\n\\n        model.eval()\\n\\n        train_correct = 0\\n        train_total = 0\\n        with torch.no_grad():\\n            for data in tqdm(train_loader):\\n                images, labels = data\\n                \\n                images = images.to(device)\\n                labels = labels.to(device)\\n\\n                outputs = model(images)\\n                _, predicted = torch.max(outputs.data, 1)\\n\\n                train_total += labels.size(0)\\n\\n                train_correct += (predicted == labels).sum().item()\\n        print(train_correct, train_total)\\n                \\n        #str(epoch) if just starting to train\\n        #str(epoch+epoch_before+1) afterwards\\n        print(\"epoch: \" + str(epoch))\\n        print(\\'Top One Error of the network on train images: %d %%\\' % (\\n                100 * (1 - float(train_correct) / train_total)))\\n\\n\\n        val_correct = 0\\n        val_total = 0\\n        with torch.no_grad():\\n            for data in tqdm(val_loader):\\n                images, labels = data\\n\\n                images = images.to(device)\\n                labels = labels.to(device)\\n\\n                outputs = model(images)\\n                _, predicted = torch.max(outputs.data, 1)\\n\\n                val_total += labels.size(0)\\n\\n                val_correct += (predicted == labels).sum().item()\\n        print(val_correct, val_total)\\n\\n        #str(epoch) if just starting to train\\n        #str(epoch+epoch_before+1) afterwards\\n        print(\"epoch: \" + str(epoch))\\n        print(\\'Top One Error of the network on validation images: %d %%\\' % (\\n                100 * (1 - float(val_correct) / val_total)))\\n        \\n\\n        gc.collect()'"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:23:17.593205Z","iopub.execute_input":"2024-07-25T04:23:17.593533Z","iopub.status.idle":"2024-07-25T04:23:17.605310Z","shell.execute_reply.started":"2024-07-25T04:23:17.593503Z","shell.execute_reply":"2024-07-25T04:23:17.604324Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-25T04:23:17.606329Z","iopub.execute_input":"2024-07-25T04:23:17.606594Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  2%|▏         | 35/1440 [04:50<3:06:27,  7.96s/it]","output_type":"stream"}]}]}